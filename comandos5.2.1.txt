# Ejercicio 5.2.1: Crear un DataFrame con todas las tablas y sus datos de MariaDB

# Comandos para reiniciar y crear los contenedores desde cero (coherente con la práctica 5.1)

# 1. Eliminar contenedores y red anteriores (si existen):
docker rm -f spark-master-custom hadoop-namenode mariadb-moviebind || true
docker network rm spark-network || true

# 2. Crear la red Docker:
docker network create spark-network
# Crea una red llamada spark-network para la comunicación entre contenedores.

# 3. Construir las imágenes personalizadas:
docker build -t spark-custom:3.3.2 ./spark-docker
# Imagen de Spark con JDBC y dependencias.
docker build -t hadoop-custom:3.3.2 ./hadoop-docker
# Imagen de Hadoop.
docker build -t my-mariadb ./mariadb-docker
# Imagen de MariaDB con datos precargados.

# 4. Iniciar los contenedores:
docker run -d --name mariadb-moviebind --network spark-network -p 3306:3306 my-mariadb
# MariaDB con datos de la práctica anterior.
docker run -d --name spark-master-custom --network spark-network -p 8080:8080 -p 7077:7077 -v "${PWD}:/opt/spark/data" spark-custom:3.3.2 tail -f /dev/null
# Spark Master.
docker run -d --name hadoop-namenode --network spark-network hadoop-custom:3.3.2 tail -f /dev/null
# Hadoop NameNode.

# 5. (Opcional) Formatear HDFS si es la primera vez:
docker exec -it hadoop-namenode bash -c "/opt/hadoop/bin/hdfs namenode -format"

# 6. Iniciar servicios HDFS dentro del contenedor Hadoop:
docker exec -it hadoop-namenode bash
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
/opt/hadoop/sbin/hadoop-daemon.sh start namenode
/opt/hadoop/sbin/hadoop-daemon.sh start datanode
/opt/hadoop/sbin/hadoop-daemon.sh start secondarynamenode
exit

# 7. Verificar que los servicios están activos y los puertos expuestos:
docker ps
# Comprueba que los contenedores están en ejecución.

# Así el entorno queda limpio y listo para ejecutar los ejercicios de la práctica 5.2.

# Descripción: Este script conecta Spark con MariaDB y crea un DataFrame para cada tabla de la base de datos moviebind.
# Requiere que el contenedor Spark tenga el conector JDBC de MariaDB y que MariaDB esté corriendo y accesible.

# 1. Ejecuta el contenedor de Spark (si no está corriendo):
docker run -it --rm --name spark --network="host" my-spark /bin/bash

# 2. Dentro del contenedor Spark, crea un archivo read_all_tables.py con el siguiente contenido (ajusta los nombres de las tablas):

# --- read_all_tables.py ---
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ReadMariaDB").getOrCreate()

url = "jdbc:mariadb://localhost:3306/moviebind"
properties = {
    "user": "user",
    "password": "1234",
    "driver": "org.mariadb.jdbc.Driver"
}

# Cambia estos nombres por los de tus tablas reales en init.sql
tables = ["tabla1", "tabla2", "tabla3"]

dataframes = {}
for table in tables:
    df = spark.read.jdbc(url=url, table=table, properties=properties)
    dataframes[table] = df
    df.show()
# ---

# 3. Ejecuta el script en Spark:
spark-submit read_all_tables.py

# Esto creará un DataFrame por cada tabla de MariaDB y mostrará sus datos.